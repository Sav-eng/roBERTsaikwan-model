{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "inference roBERTsaikwan.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3xsp36orlBG"
      },
      "source": [
        "## Install the packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Euk1H_9WrYpf"
      },
      "source": [
        "!pip install -Uqq datasets pythainlp==2.2.4 transformers==4.4.0 tensorflow==2.4.0 tensorflow_text emoji seqeval sentencepiece fuzzywuzzy\n",
        "!npx degit --force https://github.com/vistec-AI/thai2transformers#dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Yu6YuS-rr4o"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import pythainlp, transformers\n",
        "pythainlp.__version__, transformers.__version__ #fix pythainlp to stabilize word tokenization for metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3OLvYILrt0d"
      },
      "source": [
        "import collections\n",
        "import logging\n",
        "import pprint\n",
        "import re\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "#datasets\n",
        "from datasets import (\n",
        "    load_dataset, \n",
        "    load_metric, \n",
        "    concatenate_datasets,\n",
        "    load_from_disk,\n",
        ")\n",
        "\n",
        "#transformers\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForQuestionAnswering,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    default_data_collator,\n",
        ")\n",
        "\n",
        "#thai2transformers\n",
        "import thai2transformers\n",
        "from thai2transformers.metrics import (\n",
        "    squad_newmm_metric,\n",
        "    question_answering_metrics,\n",
        ")\n",
        "from thai2transformers.preprocess import (\n",
        "    prepare_qa_train_features\n",
        ")\n",
        "from thai2transformers.tokenizers import (\n",
        "    ThaiRobertaTokenizer,\n",
        "    ThaiWordsNewmmTokenizer,\n",
        "    ThaiWordsSyllableTokenizer,\n",
        "    FakeSefrCutTokenizer,\n",
        "    SEFR_SPLIT_TOKEN\n",
        ")\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMZTqEQwrvbe"
      },
      "source": [
        "model_names = [\n",
        "    'wangchanberta-base-att-spm-uncased',\n",
        "    'xlm-roberta-base',\n",
        "    'bert-base-multilingual-cased',\n",
        "    'wangchanberta-base-wiki-newmm',\n",
        "    'wangchanberta-base-wiki-ssg',\n",
        "    'wangchanberta-base-wiki-sefr',\n",
        "    'wangchanberta-base-wiki-spm',\n",
        "]\n",
        "\n",
        "tokenizers = {\n",
        "    'wangchanberta-base-att-spm-uncased': AutoTokenizer,\n",
        "    'xlm-roberta-base': AutoTokenizer,\n",
        "    'bert-base-multilingual-cased': AutoTokenizer,\n",
        "    'wangchanberta-base-wiki-newmm': ThaiWordsNewmmTokenizer,\n",
        "    'wangchanberta-base-wiki-ssg': ThaiWordsSyllableTokenizer,\n",
        "    'wangchanberta-base-wiki-sefr': FakeSefrCutTokenizer,\n",
        "    'wangchanberta-base-wiki-spm': ThaiRobertaTokenizer,\n",
        "}\n",
        "public_models = ['xlm-roberta-base', 'bert-base-multilingual-cased'] \n",
        "#@title Choose Pretrained Model\n",
        "model_name = \"wangchanberta-base-att-spm-uncased\" #@param [\"wangchanberta-base-att-spm-uncased\", \"xlm-roberta-base\", \"bert-base-multilingual-cased\", \"wangchanberta-base-wiki-newmm\", \"wangchanberta-base-wiki-syllable\", \"wangchanberta-base-wiki-sefr\", \"wangchanberta-base-wiki-spm\"]\n",
        "\n",
        "#create tokenizer\n",
        "tokenizer = tokenizers[model_name].from_pretrained(\n",
        "                f'airesearch/{model_name}' if model_name not in public_models else f'{model_name}',\n",
        "                revision='main',\n",
        "                model_max_length=416,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1vekNggxHC4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-Kws65rxLQX"
      },
      "source": [
        "## Loading the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A96yVQ23xj5P"
      },
      "source": [
        "model = AutoModelForQuestionAnswering.from_pretrained('path/to/mode')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_qzpAQXxQKv"
      },
      "source": [
        "batch_size = 16\n",
        "learning_rate = 4e-5\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"finetune_thaiSum\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=learning_rate,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size*2,\n",
        "    num_train_epochs=6,\n",
        "    warmup_ratio=0.15,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        "    save_total_limit=3,\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "data_collator = default_data_collator\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29pnklBWytro"
      },
      "source": [
        "### Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2MiqoLQywvw"
      },
      "source": [
        "def tokenize_with_space(texts, tokenizer):\n",
        "  output = []\n",
        "  encoded_texts = tokenizer(texts, max_length=416, truncation=True)\n",
        "  for text in encoded_texts['input_ids']:\n",
        "    tokenized_text = \" \".join(tokenizer.convert_ids_to_tokens(text, skip_special_tokens=True))\n",
        "    if(len(tokenized_text)==0):\n",
        "      output.append(\"\")\n",
        "      continue\n",
        "    if(tokenized_text[0]==\"‚ñÅ\"): \n",
        "      tokenized_text = tokenized_text[1:]\n",
        "    output.append(tokenized_text.strip())\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WC9_TNDbyKtX"
      },
      "source": [
        "## Install the metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBXemhQjyOnI"
      },
      "source": [
        "### Rouge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7pdxXzwyhWB"
      },
      "source": [
        "!pip install rouge"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YZgx6D2yqa4"
      },
      "source": [
        "from rouge import Rouge \n",
        "rouge = Rouge()\n",
        "def cal_rouge_score(hyps, refs, get_average_f1=True):\n",
        "  '''\n",
        "  argument: cands, refs [list of string], get_average_f1=True\n",
        "  return dict of r1, r2, rl score\n",
        "  if get_average_f1 == True return mean of rouge-1, rouge-2, rouge-L\n",
        "  '''\n",
        "  r1 = dict(); r1['precision'] = []; r1['recall'] = []; r1['f1'] = []\n",
        "  r2 = dict(); r2['precision'] = []; r2['recall'] = []; r2['f1'] = []\n",
        "  rl = dict(); rl['precision'] = []; rl['recall'] = []; rl['f1'] = []\n",
        "  for hyp, ref in zip(hyps, refs):\n",
        "    score = {}\n",
        "    if(len(hyp)==0 or len(ref)==0):\n",
        "      score = {\n",
        "          'rouge-1': {\n",
        "              'p': 0,\n",
        "              'r': 0,\n",
        "              'f': 0\n",
        "          },\n",
        "          'rouge-2': {\n",
        "              'p': 0,\n",
        "              'r': 0,\n",
        "              'f': 0\n",
        "          },\n",
        "          'rouge-l': {\n",
        "              'p': 0,\n",
        "              'r': 0,\n",
        "              'f': 0\n",
        "          }\n",
        "      }\n",
        "    else: score = rouge.get_scores(hyp, ref)[0]\n",
        "    r1['precision'].append(score['rouge-1']['p'])\n",
        "    r1['recall'].append(score['rouge-1']['r'])\n",
        "    r1['f1'].append(score['rouge-1']['f'])\n",
        "    \n",
        "    r2['precision'].append(score['rouge-2']['f'])\n",
        "    r2['recall'].append(score['rouge-2']['f'])\n",
        "    r2['f1'].append(score['rouge-2']['f'])\n",
        "\n",
        "    rl['precision'].append(score['rouge-l']['f'])\n",
        "    rl['recall'].append(score['rouge-l']['f'])\n",
        "    rl['f1'].append(score['rouge-l']['f'])\n",
        "  if(get_average_f1==True): return sum(r1['f1'])/len(r1['f1']), sum(r2['f1'])/len(r2['f1']), sum(rl['f1'])/len(rl['f1'])\n",
        "  else: return r1, r2, rl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQsj1V99yKPf"
      },
      "source": [
        "def evaluate_rouge(cands, refs, tokenizer):\n",
        "  cands_tokenized = tokenize_with_space(cands, tokenizer)\n",
        "  refs_tokenized = tokenize_with_space(refs, tokenizer)\n",
        "  r1, r2, rl = cal_rouge_score(refs_tokenized, cands_tokenized)\n",
        "  return r1, r2, rl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8X0Q5hayQVP"
      },
      "source": [
        "### BERTScore"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tlk54MhByTG_"
      },
      "source": [
        "!pip install bert_score==0.3.7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t56Il0aGyZZI"
      },
      "source": [
        "from bert_score import score\n",
        "import numpy as np\n",
        "import gc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzwsxD_fybn4"
      },
      "source": [
        "def cal_bert_score(cands, refs, get_average_f1=True):\n",
        "  '''\n",
        "  arguments: cands, refs\n",
        "  return array of presicion, recall, f1, presicion_average, recall_average, f1_average\n",
        "  if get_average == True return mean of BERTScore\n",
        "  '''\n",
        "  p, r, f1 = score(cands, refs, lang=\"others\", verbose=False)\n",
        "  p = p.numpy()\n",
        "  r = r.numpy()\n",
        "  f1 = f1.numpy()\n",
        "  if(get_average_f1==True): return f1.mean()\n",
        "  else: return p, r, f1\n",
        "\n",
        "def cal_batch_bert_score(cands, refs, get_average_f1=True, batch_size=8):\n",
        "  f1_average = []\n",
        "  for i in tqdm(range(0,len(cands),batch_size)):\n",
        "    cand_batch = cands[i:i+batch_size]\n",
        "    ref_batch = refs[i:i+batch_size]\n",
        "    res = cal_bert_score(cand_batch, ref_batch)\n",
        "    f1_average.append(res)\n",
        "    gc.collect()\n",
        "  print(f1_average)\n",
        "  return sum(f1_average)/len(f1_average)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxQBtUxFyc8H"
      },
      "source": [
        "%%time\n",
        "refs = ['‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 6 ‡∏°.‡∏Ñ.60 ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÄ‡∏ô‡∏µ‡∏¢‡∏ö‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏• ‡∏ô‡∏≤‡∏¢‡∏ß‡∏¥‡∏©‡∏ì‡∏∏ ‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏á‡∏≤‡∏° ‡∏£‡∏≠‡∏á‡∏ô‡∏≤‡∏¢‡∏Å‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ ‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏ñ‡∏∂‡∏á‡∏Å‡∏£‡∏ì‡∏µ ‡∏ó‡∏µ‡πà ‡∏ô‡∏≤‡∏¢‡∏™‡∏∏‡∏£‡∏ä‡∏±‡∏¢ ‡πÄ‡∏•‡∏µ‡πâ‡∏¢‡∏á‡∏ö‡∏∏‡∏ç‡πÄ‡∏•‡∏¥‡∏®‡∏ä‡∏±‡∏¢ ‡∏£‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô‡∏™‡∏†‡∏≤‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥ (‡∏™‡∏ô‡∏ä.) ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤ ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡πâ‡∏á‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ‡∏ñ‡∏∂‡∏á‡∏õ‡∏µ 2561 ‡∏ß‡πà‡∏≤ ‡∏Ç‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏õ‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°‡∏Å‡∏±‡∏ö ‡∏™‡∏ô‡∏ä. ‡πÅ‡∏ï‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡πà‡∏≤‡∏Ñ‡∏á‡πÑ‡∏°‡πà‡∏Å‡∏•‡πâ‡∏≤‡∏û‡∏π‡∏î‡∏≠‡∏µ‡∏Å ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ñ‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ú‡∏¥‡∏î ‡∏ã‡∏∂‡πà‡∏á‡∏ó‡∏µ‡πà ‡∏™‡∏ô‡∏ä.‡∏û‡∏π‡∏î‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡∏Å‡∏Å‡∏±‡∏ö‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£‡∏£‡πà‡∏≤‡∏á‡∏£‡∏±‡∏ê‡∏ò‡∏£‡∏£‡∏°‡∏ô‡∏π‡∏ç(‡∏Å‡∏£‡∏ò.) ‡∏ï‡∏ô‡∏à‡∏∂‡∏á‡πÑ‡∏°‡πà‡∏Ç‡∏≠‡∏ß‡∏¥‡∏û‡∏≤‡∏Å‡∏©‡πå‡∏ß‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡πå ‡πÅ‡∏ï‡πà‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏ß‡πà‡∏≤‡∏¢‡∏±‡∏á‡πÄ‡∏î‡∏¥‡∏ô‡∏ï‡∏≤‡∏°‡πÇ‡∏£‡∏î‡πÅ‡∏°‡πá‡∏õ ‡∏ã‡∏∂‡πà‡∏á‡πÇ‡∏£‡∏î‡πÅ‡∏°‡πá‡∏õ‡∏°‡∏≠‡∏á‡πÑ‡∏î‡πâ‡∏™‡∏≠‡∏á‡πÅ‡∏ö‡∏ö ‡∏Ñ‡∏∑‡∏≠ ‡∏°‡∏µ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤ ‡πÇ‡∏î‡∏¢‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡πÉ‡∏ä‡πâ‡∏£‡∏±‡∏ê‡∏ò‡∏£‡∏£‡∏°‡∏ô‡∏π‡∏ç ‡πÅ‡∏ï‡πà‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ‡∏£‡∏±‡∏ê‡∏ò‡∏£‡∏£‡∏°‡∏ô‡∏π‡∏ç‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡πÉ‡∏ä‡πâ ‡∏à‡∏∂‡∏á‡∏¢‡∏±‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ô‡∏±‡∏ö‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡πà‡∏≤‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏£‡πà‡∏≤‡∏á‡∏£‡∏±‡∏ê‡∏ò‡∏£‡∏£‡∏°‡∏ô‡∏π‡∏ç‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏•‡∏π‡∏Å ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô 240 ‡∏ß‡∏±‡∏ô ‡∏Å‡πà‡∏≠‡∏ô‡∏à‡∏∞‡∏™‡πà‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÉ‡∏´‡πâ ‡∏™‡∏ô‡∏ä.‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤ ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô 2 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô\\xa0,‡∏ô‡∏≤‡∏¢‡∏ß‡∏¥‡∏©‡∏ì‡∏∏ ‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏ï‡πà‡∏≠‡∏ß‡πà‡∏≤ ‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Å‡πá‡∏à‡∏∞‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö ‡∏Å‡∏£‡∏ò.‡∏≠‡∏µ‡∏Å 1 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô ‡∏Å‡πà‡∏≠‡∏ô‡∏ô‡∏≥‡∏Ç‡∏∂‡πâ‡∏ô‡∏ó‡∏π‡∏•‡πÄ‡∏Å‡∏•‡πâ‡∏≤‡∏Ø ‡∏ó‡∏£‡∏á‡∏•‡∏á‡∏û‡∏£‡∏∞‡∏õ‡∏£‡∏°‡∏≤‡∏†‡∏¥‡πÑ‡∏ò‡∏¢ ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô 90 ‡∏ß‡∏±‡∏ô ‡πÅ‡∏•‡∏∞‡∏à‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡πâ‡∏á‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤ 5 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô ‡∏ã‡∏∂‡πà‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡πÇ‡∏£‡∏î‡πÅ‡∏°‡πá‡∏õ‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏¥‡∏°‡∏≠‡∏¢‡∏π‡πà ‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ß‡∏±‡∏ô‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏ß‡πâ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏õ‡∏µ 60 ‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡πÅ‡∏ï‡πà‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏ß‡∏£‡∏£‡∏Ñ‡∏ï‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏à‡∏∂‡∏á‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ ‡∏™‡πà‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏•‡∏π‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î 4 ‡∏â‡∏ö‡∏±‡∏ö ‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ‡∏Å‡∏£‡∏ò.‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏™‡∏£‡πá‡∏à 2 ‡∏â‡∏ö‡∏±‡∏ö ‡∏Ñ‡∏∑‡∏≠ ‡∏û.‡∏£.‡∏õ.‡∏û‡∏£‡∏£‡∏Ñ‡∏Å‡∏≤‡∏£‡πÄ‡∏°‡∏∑‡∏≠‡∏á ‡πÅ‡∏•‡∏∞‡∏û.‡∏£.‡∏õ. ‡∏Ñ‡∏ì‡∏∞‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡πâ‡∏á ‡πÅ‡∏ï‡πà ‡∏û.‡∏£.‡∏õ.‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡∏ß‡∏£‡∏à‡∏∞‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏•‡∏±‡∏ö‡∏•‡πà‡∏≤‡∏ä‡πâ‡∏≤ ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô ‡∏Å‡∏£‡∏ò.‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏≠‡∏Å‡∏ä‡∏µ‡πâ‡πÅ‡∏à‡∏á‡∏ñ‡∏∂‡∏á‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏°‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏î‡∏±‡∏á‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏•‡πà‡∏≤‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤‡∏Å‡∏≥‡∏´‡∏ô‡∏î ‡∏™‡πà‡∏á‡∏ú‡∏•‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏™‡∏á‡∏™‡∏±‡∏¢‡∏à‡∏ô‡∏ñ‡∏∂‡∏á‡∏ó‡∏∏‡∏Å‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ ‡∏™‡πà‡∏ß‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà ‡∏™‡∏ô‡∏ä. ‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤ ‡∏°‡∏µ‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏Å‡∏≤‡∏£‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏Ç‡∏≠‡∏á ‡∏™‡∏ô‡∏ä.‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠‡πÇ‡∏£‡∏î‡πÅ‡∏°‡πá‡∏õ‡∏ô‡∏±‡πâ‡∏ô ‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•‡πÄ‡∏Ñ‡∏¢‡∏ö‡∏≠‡∏Å‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß‡∏ß‡πà‡∏≤‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏ô‡∏µ‡πâ‡∏Ç‡∏≠‡∏á‡πÇ‡∏£‡∏î‡πÅ‡∏°‡πá‡∏õ‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏à‡∏∞‡πÄ‡∏¢‡∏≠‡∏∞‡∏Å‡∏ß‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏°‡∏≤ ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô ‡∏™‡∏ô‡∏ä.‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÑ‡∏î‡πâ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å ‡∏™‡∏ô‡∏ä.‡πÉ‡∏´‡πâ‡πÅ‡∏•‡πâ‡∏ß.']\n",
        "cands = ['‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 6 ‡∏°.‡∏Ñ.60 ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÄ‡∏ô‡∏µ‡∏¢‡∏ö‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏• ‡∏ô‡∏≤‡∏¢‡∏ß‡∏¥‡∏©‡∏ì‡∏∏ ‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏á‡∏≤‡∏° ‡∏£‡∏≠‡∏á‡∏ô‡∏≤‡∏¢‡∏Å‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ ‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏ñ‡∏∂‡∏á‡∏Å‡∏£‡∏ì‡∏µ ‡∏ó‡∏µ‡πà ‡∏ô‡∏≤‡∏¢‡∏™‡∏∏‡∏£‡∏ä‡∏±‡∏¢ ‡πÄ‡∏•‡∏µ‡πâ‡∏¢‡∏á‡∏ö‡∏∏‡∏ç‡πÄ‡∏•‡∏¥‡∏®‡∏ä‡∏±‡∏¢ ‡∏£‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô‡∏™‡∏†‡∏≤‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥ (‡∏™‡∏ô‡∏ä.)']\n",
        "f1_average = cal_bert_score(cands, refs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwsWs5RtyeWo"
      },
      "source": [
        "print(f1_average)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktzjeDenzeDh"
      },
      "source": [
        "## Train and evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVvP4VfIz2SG"
      },
      "source": [
        "Download the data and make it to datasets `dataset`.\n",
        "The dataset should consist of `attention_mask` and `input_ids`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7i_cQFRhyD7-"
      },
      "source": [
        "raw_predictions = trainer.predict(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz19VxNvzZOh"
      },
      "source": [
        "predictions = post_process_index(tokenized_datasets['test'], raw_predictions[0], tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlxQut7TzdSo"
      },
      "source": [
        "r1, r2, rl = evaluate_rouge(predictions, gold_summaries, tokenizer)\n",
        "print(r1, r2, rl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uPCPBdqziRY"
      },
      "source": [
        "BERTScore = cal_batch_bert_score(predictions, gold_summaries, tokenizer, batch_size=128)\n",
        "print(BERTScore)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}