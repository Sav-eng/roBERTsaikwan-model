{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "inference roBERTsaikwan.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3xsp36orlBG"
      },
      "source": [
        "## Install the packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Euk1H_9WrYpf"
      },
      "source": [
        "!pip install -Uqq datasets pythainlp==2.2.4 transformers==4.4.0 tensorflow==2.4.0 tensorflow_text emoji seqeval sentencepiece fuzzywuzzy\n",
        "!npx degit --force https://github.com/vistec-AI/thai2transformers#dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Yu6YuS-rr4o"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import pythainlp, transformers\n",
        "pythainlp.__version__, transformers.__version__ #fix pythainlp to stabilize word tokenization for metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3OLvYILrt0d"
      },
      "source": [
        "import collections\n",
        "import logging\n",
        "import pprint\n",
        "import re\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "#datasets\n",
        "from datasets import (\n",
        "    load_dataset, \n",
        "    load_metric, \n",
        "    concatenate_datasets,\n",
        "    load_from_disk,\n",
        ")\n",
        "\n",
        "#transformers\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForQuestionAnswering,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    default_data_collator,\n",
        ")\n",
        "\n",
        "#thai2transformers\n",
        "import thai2transformers\n",
        "from thai2transformers.metrics import (\n",
        "    squad_newmm_metric,\n",
        "    question_answering_metrics,\n",
        ")\n",
        "from thai2transformers.preprocess import (\n",
        "    prepare_qa_train_features\n",
        ")\n",
        "from thai2transformers.tokenizers import (\n",
        "    ThaiRobertaTokenizer,\n",
        "    ThaiWordsNewmmTokenizer,\n",
        "    ThaiWordsSyllableTokenizer,\n",
        "    FakeSefrCutTokenizer,\n",
        "    SEFR_SPLIT_TOKEN\n",
        ")\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMZTqEQwrvbe"
      },
      "source": [
        "model_names = [\n",
        "    'wangchanberta-base-att-spm-uncased',\n",
        "    'xlm-roberta-base',\n",
        "    'bert-base-multilingual-cased',\n",
        "    'wangchanberta-base-wiki-newmm',\n",
        "    'wangchanberta-base-wiki-ssg',\n",
        "    'wangchanberta-base-wiki-sefr',\n",
        "    'wangchanberta-base-wiki-spm',\n",
        "]\n",
        "\n",
        "tokenizers = {\n",
        "    'wangchanberta-base-att-spm-uncased': AutoTokenizer,\n",
        "    'xlm-roberta-base': AutoTokenizer,\n",
        "    'bert-base-multilingual-cased': AutoTokenizer,\n",
        "    'wangchanberta-base-wiki-newmm': ThaiWordsNewmmTokenizer,\n",
        "    'wangchanberta-base-wiki-ssg': ThaiWordsSyllableTokenizer,\n",
        "    'wangchanberta-base-wiki-sefr': FakeSefrCutTokenizer,\n",
        "    'wangchanberta-base-wiki-spm': ThaiRobertaTokenizer,\n",
        "}\n",
        "public_models = ['xlm-roberta-base', 'bert-base-multilingual-cased'] \n",
        "#@title Choose Pretrained Model\n",
        "model_name = \"wangchanberta-base-att-spm-uncased\" #@param [\"wangchanberta-base-att-spm-uncased\", \"xlm-roberta-base\", \"bert-base-multilingual-cased\", \"wangchanberta-base-wiki-newmm\", \"wangchanberta-base-wiki-syllable\", \"wangchanberta-base-wiki-sefr\", \"wangchanberta-base-wiki-spm\"]\n",
        "\n",
        "#create tokenizer\n",
        "tokenizer = tokenizers[model_name].from_pretrained(\n",
        "                f'airesearch/{model_name}' if model_name not in public_models else f'{model_name}',\n",
        "                revision='main',\n",
        "                model_max_length=416,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1vekNggxHC4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-Kws65rxLQX"
      },
      "source": [
        "## Loading the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A96yVQ23xj5P"
      },
      "source": [
        "model = AutoModelForQuestionAnswering.from_pretrained('path/to/mode')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_qzpAQXxQKv"
      },
      "source": [
        "batch_size = 16\n",
        "learning_rate = 4e-5\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"finetune_thaiSum\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=learning_rate,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size*2,\n",
        "    num_train_epochs=6,\n",
        "    warmup_ratio=0.15,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        "    save_total_limit=3,\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "data_collator = default_data_collator\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29pnklBWytro"
      },
      "source": [
        "### Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2MiqoLQywvw"
      },
      "source": [
        "def tokenize_with_space(texts, tokenizer):\n",
        "  output = []\n",
        "  encoded_texts = tokenizer(texts, max_length=416, truncation=True)\n",
        "  for text in encoded_texts['input_ids']:\n",
        "    tokenized_text = \" \".join(tokenizer.convert_ids_to_tokens(text, skip_special_tokens=True))\n",
        "    if(len(tokenized_text)==0):\n",
        "      output.append(\"\")\n",
        "      continue\n",
        "    if(tokenized_text[0]==\"▁\"): \n",
        "      tokenized_text = tokenized_text[1:]\n",
        "    output.append(tokenized_text.strip())\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WC9_TNDbyKtX"
      },
      "source": [
        "## Install the metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBXemhQjyOnI"
      },
      "source": [
        "### Rouge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7pdxXzwyhWB"
      },
      "source": [
        "!pip install rouge"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YZgx6D2yqa4"
      },
      "source": [
        "from rouge import Rouge \n",
        "rouge = Rouge()\n",
        "def cal_rouge_score(hyps, refs, get_average_f1=True):\n",
        "  '''\n",
        "  argument: cands, refs [list of string], get_average_f1=True\n",
        "  return dict of r1, r2, rl score\n",
        "  if get_average_f1 == True return mean of rouge-1, rouge-2, rouge-L\n",
        "  '''\n",
        "  r1 = dict(); r1['precision'] = []; r1['recall'] = []; r1['f1'] = []\n",
        "  r2 = dict(); r2['precision'] = []; r2['recall'] = []; r2['f1'] = []\n",
        "  rl = dict(); rl['precision'] = []; rl['recall'] = []; rl['f1'] = []\n",
        "  for hyp, ref in zip(hyps, refs):\n",
        "    score = {}\n",
        "    if(len(hyp)==0 or len(ref)==0):\n",
        "      score = {\n",
        "          'rouge-1': {\n",
        "              'p': 0,\n",
        "              'r': 0,\n",
        "              'f': 0\n",
        "          },\n",
        "          'rouge-2': {\n",
        "              'p': 0,\n",
        "              'r': 0,\n",
        "              'f': 0\n",
        "          },\n",
        "          'rouge-l': {\n",
        "              'p': 0,\n",
        "              'r': 0,\n",
        "              'f': 0\n",
        "          }\n",
        "      }\n",
        "    else: score = rouge.get_scores(hyp, ref)[0]\n",
        "    r1['precision'].append(score['rouge-1']['p'])\n",
        "    r1['recall'].append(score['rouge-1']['r'])\n",
        "    r1['f1'].append(score['rouge-1']['f'])\n",
        "    \n",
        "    r2['precision'].append(score['rouge-2']['f'])\n",
        "    r2['recall'].append(score['rouge-2']['f'])\n",
        "    r2['f1'].append(score['rouge-2']['f'])\n",
        "\n",
        "    rl['precision'].append(score['rouge-l']['f'])\n",
        "    rl['recall'].append(score['rouge-l']['f'])\n",
        "    rl['f1'].append(score['rouge-l']['f'])\n",
        "  if(get_average_f1==True): return sum(r1['f1'])/len(r1['f1']), sum(r2['f1'])/len(r2['f1']), sum(rl['f1'])/len(rl['f1'])\n",
        "  else: return r1, r2, rl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQsj1V99yKPf"
      },
      "source": [
        "def evaluate_rouge(cands, refs, tokenizer):\n",
        "  cands_tokenized = tokenize_with_space(cands, tokenizer)\n",
        "  refs_tokenized = tokenize_with_space(refs, tokenizer)\n",
        "  r1, r2, rl = cal_rouge_score(refs_tokenized, cands_tokenized)\n",
        "  return r1, r2, rl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8X0Q5hayQVP"
      },
      "source": [
        "### BERTScore"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tlk54MhByTG_"
      },
      "source": [
        "!pip install bert_score==0.3.7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t56Il0aGyZZI"
      },
      "source": [
        "from bert_score import score\n",
        "import numpy as np\n",
        "import gc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzwsxD_fybn4"
      },
      "source": [
        "def cal_bert_score(cands, refs, get_average_f1=True):\n",
        "  '''\n",
        "  arguments: cands, refs\n",
        "  return array of presicion, recall, f1, presicion_average, recall_average, f1_average\n",
        "  if get_average == True return mean of BERTScore\n",
        "  '''\n",
        "  p, r, f1 = score(cands, refs, lang=\"others\", verbose=False)\n",
        "  p = p.numpy()\n",
        "  r = r.numpy()\n",
        "  f1 = f1.numpy()\n",
        "  if(get_average_f1==True): return f1.mean()\n",
        "  else: return p, r, f1\n",
        "\n",
        "def cal_batch_bert_score(cands, refs, get_average_f1=True, batch_size=8):\n",
        "  f1_average = []\n",
        "  for i in tqdm(range(0,len(cands),batch_size)):\n",
        "    cand_batch = cands[i:i+batch_size]\n",
        "    ref_batch = refs[i:i+batch_size]\n",
        "    res = cal_bert_score(cand_batch, ref_batch)\n",
        "    f1_average.append(res)\n",
        "    gc.collect()\n",
        "  print(f1_average)\n",
        "  return sum(f1_average)/len(f1_average)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxQBtUxFyc8H"
      },
      "source": [
        "%%time\n",
        "refs = ['เมื่อวันที่ 6 ม.ค.60 ที่ทำเนียบรัฐบาล นายวิษณุ เครืองาม รองนายกรัฐมนตรี กล่าวถึงกรณี ที่ นายสุรชัย เลี้ยงบุญเลิศชัย รองประธานสภานิติบัญญัติแห่งชาติ (สนช.) ออกมาระบุว่า การเลือกตั้งจะถูกเลื่อนออกไปถึงปี 2561 ว่า ขอให้ไปสอบถามกับ สนช. แต่เชื่อว่าคงไม่กล้าพูดอีก เพราะทำให้คนเข้าใจผิด ซึ่งที่ สนช.พูดเนื่องจากผูกกับกฎหมายของกรรมการร่างรัฐธรรมนูญ(กรธ.) ตนจึงไม่ขอวิพากษ์วิจารณ์ แต่รัฐบาลยืนยันว่ายังเดินตามโรดแม็ป ซึ่งโรดแม็ปมองได้สองแบบ คือ มีลำดับขั้นตอนและการกำหนดช่วงเวลา โดยเริ่มต้นจากการประกาศใช้รัฐธรรมนูญ แต่ขณะนี้รัฐธรรมนูญยังไม่ประกาศใช้ จึงยังเริ่มนับหนึ่งไม่ถูก จากนั้นเข้าสู่ขั้นตอนการร่างกฎหมายประกอบร่างรัฐธรรมนูญหรือกฎหมายลูก ภายใน 240 วัน ก่อนจะส่งกลับให้ สนช.พิจารณา ภายใน 2 เดือน\\xa0,นายวิษณุ กล่าวต่อว่า หากมีการแก้ไขก็จะมีการพิจารณาร่วมกับ กรธ.อีก 1 เดือน ก่อนนำขึ้นทูลเกล้าฯ ทรงลงพระปรมาภิไธย ภายใน 90 วัน และจะเข้าสู่การเลือกตั้งภายในระยะเวลา 5 เดือน ซึ่งทั้งหมดนี้คือโรดแม็ปที่ยังเป็นแบบเดิมอยู่ ส่วนเดิมที่กำหนดวันเลือกตั้งไว้ภายในปี 60 นั้น เพราะมาจากสมมติฐานของขั้นตอนเดิมทั้งหมด แต่เมื่อมีเหตุสวรรคตทุกอย่างจึงต้องเลื่อนออกไป ส่วนการพิจารณากฎหมายลูกทั้งหมด 4 ฉบับ ขณะนี้กรธ.พิจารณาแล้วเสร็จ 2 ฉบับ คือ พ.ร.ป.พรรคการเมือง และพ.ร.ป. คณะกรรมการการเลือกตั้ง แต่ พ.ร.ป.การเลือกตั้งควรจะพิจารณาได้เร็วกลับล่าช้า ดังนั้น กรธ.จะต้องออกชี้แจงถึงเหตุผลว่าทำไมพิจารณากฎหมายดังกล่าวล่าช้ากว่ากำหนด ส่งผลให้เกิดข้อสงสัยจนถึงทุกวันนี้ ส่วนกรณีที่ สนช. ระบุว่า มีกฎหมายเข้าสู่การพิจารณาของ สนช.เป็นจำนวนมาก ทำให้ส่งผลกระทบต่อโรดแม็ปนั้น รัฐบาลเคยบอกไว้แล้วว่าในช่วงนี้ของโรดแม็ปกฎหมายจะเยอะกว่าที่ผ่านมา ดังนั้น สนช.จะต้องบริหารจัดการกันเอง เพราะได้มีการเพิ่มสมาชิก สนช.ให้แล้ว.']\n",
        "cands = ['เมื่อวันที่ 6 ม.ค.60 ที่ทำเนียบรัฐบาล นายวิษณุ เครืองาม รองนายกรัฐมนตรี กล่าวถึงกรณี ที่ นายสุรชัย เลี้ยงบุญเลิศชัย รองประธานสภานิติบัญญัติแห่งชาติ (สนช.)']\n",
        "f1_average = cal_bert_score(cands, refs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwsWs5RtyeWo"
      },
      "source": [
        "print(f1_average)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktzjeDenzeDh"
      },
      "source": [
        "## Train and evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVvP4VfIz2SG"
      },
      "source": [
        "Download the data and make it to datasets `dataset`.\n",
        "The dataset should consist of `attention_mask` and `input_ids`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7i_cQFRhyD7-"
      },
      "source": [
        "raw_predictions = trainer.predict(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz19VxNvzZOh"
      },
      "source": [
        "predictions = post_process_index(tokenized_datasets['test'], raw_predictions[0], tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlxQut7TzdSo"
      },
      "source": [
        "r1, r2, rl = evaluate_rouge(predictions, gold_summaries, tokenizer)\n",
        "print(r1, r2, rl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uPCPBdqziRY"
      },
      "source": [
        "BERTScore = cal_batch_bert_score(predictions, gold_summaries, tokenizer, batch_size=128)\n",
        "print(BERTScore)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}