{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fine-tuning roBERTsaikwan.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3xsp36orlBG"
      },
      "source": [
        "## Install the packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Euk1H_9WrYpf"
      },
      "source": [
        "!pip install -Uqq datasets pythainlp==2.2.4 transformers==4.4.0 tensorflow==2.4.0 tensorflow_text emoji seqeval sentencepiece fuzzywuzzy\n",
        "!npx degit --force https://github.com/vistec-AI/thai2transformers#dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Yu6YuS-rr4o"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import pythainlp, transformers\n",
        "pythainlp.__version__, transformers.__version__ #fix pythainlp to stabilize word tokenization for metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3OLvYILrt0d"
      },
      "source": [
        "import collections\n",
        "import logging\n",
        "import pprint\n",
        "import re\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "#datasets\n",
        "from datasets import (\n",
        "    load_dataset, \n",
        "    load_metric, \n",
        "    concatenate_datasets,\n",
        "    load_from_disk,\n",
        ")\n",
        "\n",
        "#transformers\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForQuestionAnswering,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    default_data_collator,\n",
        ")\n",
        "\n",
        "#thai2transformers\n",
        "import thai2transformers\n",
        "from thai2transformers.metrics import (\n",
        "    squad_newmm_metric,\n",
        "    question_answering_metrics,\n",
        ")\n",
        "from thai2transformers.preprocess import (\n",
        "    prepare_qa_train_features\n",
        ")\n",
        "from thai2transformers.tokenizers import (\n",
        "    ThaiRobertaTokenizer,\n",
        "    ThaiWordsNewmmTokenizer,\n",
        "    ThaiWordsSyllableTokenizer,\n",
        "    FakeSefrCutTokenizer,\n",
        "    SEFR_SPLIT_TOKEN\n",
        ")\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMZTqEQwrvbe"
      },
      "source": [
        "model_names = [\n",
        "    'wangchanberta-base-att-spm-uncased',\n",
        "    'xlm-roberta-base',\n",
        "    'bert-base-multilingual-cased',\n",
        "    'wangchanberta-base-wiki-newmm',\n",
        "    'wangchanberta-base-wiki-ssg',\n",
        "    'wangchanberta-base-wiki-sefr',\n",
        "    'wangchanberta-base-wiki-spm',\n",
        "]\n",
        "\n",
        "tokenizers = {\n",
        "    'wangchanberta-base-att-spm-uncased': AutoTokenizer,\n",
        "    'xlm-roberta-base': AutoTokenizer,\n",
        "    'bert-base-multilingual-cased': AutoTokenizer,\n",
        "    'wangchanberta-base-wiki-newmm': ThaiWordsNewmmTokenizer,\n",
        "    'wangchanberta-base-wiki-ssg': ThaiWordsSyllableTokenizer,\n",
        "    'wangchanberta-base-wiki-sefr': FakeSefrCutTokenizer,\n",
        "    'wangchanberta-base-wiki-spm': ThaiRobertaTokenizer,\n",
        "}\n",
        "public_models = ['xlm-roberta-base', 'bert-base-multilingual-cased'] \n",
        "#@title Choose Pretrained Model\n",
        "model_name = \"wangchanberta-base-att-spm-uncased\" #@param [\"wangchanberta-base-att-spm-uncased\", \"xlm-roberta-base\", \"bert-base-multilingual-cased\", \"wangchanberta-base-wiki-newmm\", \"wangchanberta-base-wiki-syllable\", \"wangchanberta-base-wiki-sefr\", \"wangchanberta-base-wiki-spm\"]\n",
        "\n",
        "#create tokenizer\n",
        "tokenizer = tokenizers[model_name].from_pretrained(\n",
        "                f'airesearch/{model_name}' if model_name not in public_models else f'{model_name}',\n",
        "                revision='main',\n",
        "                model_max_length=416,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GbdMOFWrzZW"
      },
      "source": [
        "## Prepare function for calculate metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QNuVmpVryp9"
      },
      "source": [
        "!pip install rouge"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mqc13htTr4SW"
      },
      "source": [
        "from rouge import Rouge \n",
        "rouge = Rouge()\n",
        "def cal_rouge_score(hyps, refs, get_average_f1=True):\n",
        "  '''\n",
        "  argument: cands, refs [list of string], get_average_f1=True\n",
        "  return dict of r1, r2, rl score\n",
        "  if get_average_f1 == True return mean of rouge-1, rouge-2, rouge-L\n",
        "  '''\n",
        "  r1 = dict(); r1['precision'] = []; r1['recall'] = []; r1['f1'] = []\n",
        "  r2 = dict(); r2['precision'] = []; r2['recall'] = []; r2['f1'] = []\n",
        "  rl = dict(); rl['precision'] = []; rl['recall'] = []; rl['f1'] = []\n",
        "  for hyp, ref in zip(hyps, refs):\n",
        "    score = {}\n",
        "    if(len(hyp)==0 or len(ref)==0):\n",
        "      score = {\n",
        "          'rouge-1': {\n",
        "              'p': 0,\n",
        "              'r': 0,\n",
        "              'f': 0\n",
        "          },\n",
        "          'rouge-2': {\n",
        "              'p': 0,\n",
        "              'r': 0,\n",
        "              'f': 0\n",
        "          },\n",
        "          'rouge-l': {\n",
        "              'p': 0,\n",
        "              'r': 0,\n",
        "              'f': 0\n",
        "          }\n",
        "      }\n",
        "    else: score = rouge.get_scores(hyp, ref)[0]\n",
        "    r1['precision'].append(score['rouge-1']['p'])\n",
        "    r1['recall'].append(score['rouge-1']['r'])\n",
        "    r1['f1'].append(score['rouge-1']['f'])\n",
        "    \n",
        "    r2['precision'].append(score['rouge-2']['f'])\n",
        "    r2['recall'].append(score['rouge-2']['f'])\n",
        "    r2['f1'].append(score['rouge-2']['f'])\n",
        "\n",
        "    rl['precision'].append(score['rouge-l']['f'])\n",
        "    rl['recall'].append(score['rouge-l']['f'])\n",
        "    rl['f1'].append(score['rouge-l']['f'])\n",
        "  if(get_average_f1==True): return sum(r1['f1'])/len(r1['f1']), sum(r2['f1'])/len(r2['f1']), sum(rl['f1'])/len(rl['f1'])\n",
        "  else: return r1, r2, rl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVvp51lZr8RP"
      },
      "source": [
        "cands = ['test test test test test test bad']\n",
        "refs = ['test test']\n",
        "\n",
        "r1, r2, rl = cal_rouge_score(cands, refs)\n",
        "print(r1)\n",
        "print(r2)\n",
        "print(rl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja8_xWerr-_f"
      },
      "source": [
        "## Utility functions for calculate label in our use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm9adMU2r-hW"
      },
      "source": [
        "def tokenize_with_space(texts, tokenizer):\n",
        "  output = []\n",
        "  encoded_texts = tokenizer(texts, max_length=416, truncation=True)\n",
        "  for text in encoded_texts['input_ids']:\n",
        "    tokenized_text = \" \".join(tokenizer.convert_ids_to_tokens(text, skip_special_tokens=True))\n",
        "    if(len(tokenized_text)==0):\n",
        "      output.append(\"\")\n",
        "      continue\n",
        "    if(tokenized_text[0]==\"▁\"): \n",
        "      tokenized_text = tokenized_text[1:]\n",
        "    output.append(tokenized_text.strip())\n",
        "  return output\n",
        "\n",
        "\n",
        "def selection_start_end(paragraphs_raw, summaries_raw, tokenizer, length_sum_max = 10, metric='rouge-l'):\n",
        "  \"\"\"\n",
        "  Select the start position and end postion for each paragraph to make a summary and maximize the Rouge-L score\n",
        "  Args: \n",
        "  paragraphs [#number of paragraph, #number of word, #number of character] (must be tokenized with space and space change to '_')\n",
        "  summaries [#number of summary, #number of word, #number of character] (must be tokenized with space and space change to '_')\n",
        "  \"\"\"\n",
        "  \n",
        "  paragraphs = tokenize_with_space(paragraphs_raw, tokenizer)\n",
        "  summaries = tokenize_with_space(summaries_raw, tokenizer)\n",
        "  start_position = []\n",
        "  end_position = []\n",
        "  texts_all = []\n",
        "  for paragraph_raw, summary in zip(paragraphs, summaries):\n",
        "    paragraph = paragraph_raw.split(\" \")\n",
        "    len_paragraph = len(paragraph)\n",
        "    max_score = 0\n",
        "    s = 0\n",
        "    e = len_paragraph\n",
        "    text = \"\"\n",
        "    for length in range(1, length_sum_max):\n",
        "      for start_pos in range(len_paragraph-length+1):\n",
        "        t_summary = \" \".join(paragraph[start_pos:start_pos+length])\n",
        "        try:\n",
        "          r1, r2, score = cal_rouge_score([summary], [t_summary])\n",
        "          if(max_score < score):\n",
        "            max_score = score\n",
        "            s = start_pos\n",
        "            e = start_pos + length\n",
        "            text = \"\".join(paragraph[s:e])\n",
        "        except:\n",
        "          pass\n",
        "    start_position.append(s)\n",
        "    end_position.append(e)\n",
        "    texts_all.append(text)\n",
        "  return start_position, end_position, texts_all\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iz_sq6XasF2Y"
      },
      "source": [
        "import collections as coll\n",
        "# stopwords = pkgutil.get_data(__package__, 'smart_common_words.txt')\n",
        "# stopwords = stopwords.decode('ascii').split('\\n')\n",
        "# stopwords = {key.strip(): 1 for key in stopwords}\n",
        "\n",
        "def _get_ngrams_count(n, text):\n",
        "    \"\"\"Calcualtes n-grams.\n",
        "    Args:\n",
        "      n: which n-grams to calculate\n",
        "      text: An array of tokens\n",
        "    Returns:\n",
        "      A set of n-grams\n",
        "    \"\"\"\n",
        "    ngram_dic = coll.defaultdict(int)\n",
        "    text_length = len(text)\n",
        "    max_index_ngram_start = text_length - n\n",
        "    for i in range(max_index_ngram_start + 1):\n",
        "        ngram_dic[tuple(text[i:i + n])] += 1\n",
        "    return ngram_dic\n",
        "\n",
        "def _get_ngrams(n, text):\n",
        "    \"\"\"Calcualtes n-grams.\n",
        "    Args:\n",
        "      n: which n-grams to calculate\n",
        "      text: An array of tokens\n",
        "    Returns:\n",
        "      A set of n-grams\n",
        "    \"\"\"\n",
        "    ngram_set = set()\n",
        "    text_length = len(text)\n",
        "    max_index_ngram_start = text_length - n\n",
        "    for i in range(max_index_ngram_start + 1):\n",
        "        ngram_set.add(tuple(text[i:i + n]))\n",
        "    return ngram_set\n",
        "\n",
        "def _get_word_ngrams_list(n, text):\n",
        "    \"\"\"Calcualtes n-grams.\n",
        "    Args:\n",
        "      n: which n-grams to calculate\n",
        "      text: An array of tokens\n",
        "    Returns:\n",
        "      A set of n-grams\n",
        "    \"\"\"\n",
        "    text = sum(text, [])\n",
        "    ngram_set = []\n",
        "    text_length = len(text)\n",
        "    max_index_ngram_start = text_length - n\n",
        "    for i in range(max_index_ngram_start + 1):\n",
        "        ngram_set.append(tuple(text[i:i + n]))\n",
        "    return ngram_set\n",
        "\n",
        "def _get_word_ngrams(n, sentences, do_count=False):\n",
        "    \"\"\"Calculates word n-grams for multiple sentences.\n",
        "    \"\"\"\n",
        "    assert len(sentences) > 0\n",
        "    assert n > 0\n",
        "\n",
        "    # words = _split_into_words(sentences)\n",
        "\n",
        "    words = sum(sentences, [])\n",
        "    # words = [w for w in words if w not in stopwords]\n",
        "    if do_count:\n",
        "        return _get_ngrams_count(n, words)\n",
        "    return _get_ngrams(n, words)\n",
        "  \n",
        "def cal_rouge(evaluated_ngrams, reference_ngrams):\n",
        "    reference_count = len(reference_ngrams)\n",
        "    evaluated_count = len(evaluated_ngrams)\n",
        "\n",
        "    overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)\n",
        "    overlapping_count = len(overlapping_ngrams)\n",
        "\n",
        "    if evaluated_count == 0:\n",
        "        precision = 0.0\n",
        "    else:\n",
        "        precision = overlapping_count / evaluated_count\n",
        "\n",
        "    if reference_count == 0:\n",
        "        recall = 0.0\n",
        "    else:\n",
        "        recall = overlapping_count / reference_count\n",
        "\n",
        "    f1_score = 2.0 * ((precision * recall) / (precision + recall + 1e-8))\n",
        "    return {\"f\": f1_score, \"p\": precision, \"r\": recall}\n",
        "\n",
        "def selection_start_end_r1_r2(doc, abstract, tokenizer, summary_size = 50):\n",
        "  \"\"\"\n",
        "  Select the start position and end postion for each paragraph to make a summary and maximize the Rouge-L score\n",
        "  Args: \n",
        "  paragraphs [#number of paragraph, #number of word, #number of character] (must be tokenized with space and space change to '_')\n",
        "  summaries [#number of summary, #number of word, #number of character] (must be tokenized with space and space change to '_')\n",
        "  \"\"\"\n",
        "  \n",
        "  max_rouge = 0.0\n",
        "  tokenized_doc = tokenize_with_space([doc], tokenizer)[0].split(\" \")\n",
        "  tokenized_abstract = tokenize_with_space([abstract], tokenizer)[0].split(\" \")\n",
        "  # abstract = sum(abstract_sent_list, [])\n",
        "  # abstract = ' '.join(abstract).split()\n",
        "  # sents = [' '.join(s).split() for s in doc_sent_list]\n",
        "  evaluated_1grams = _get_word_ngrams_list(1, [tokenized_doc])\n",
        "  reference_1grams = _get_word_ngrams(1, [tokenized_abstract])\n",
        "  evaluated_2grams = _get_word_ngrams_list(2, [tokenized_doc])\n",
        "  reference_2grams = _get_word_ngrams(2, [tokenized_abstract])\n",
        "\n",
        "\n",
        "  start = 0\n",
        "  end = 0\n",
        "  text = \"\"\n",
        "  max_rouge = 0\n",
        "  for s in range(1,summary_size):\n",
        "      for i in range(len(tokenized_doc)-s+1):\n",
        "          # if (i in selected):\n",
        "          #     continue\n",
        "          c = range(i,i+s)\n",
        "          candidates_1 = set(evaluated_1grams[i:i+s])\n",
        "          # candidates_1 = set.union(*map(set, candidates_1))\n",
        "          rouge = cal_rouge(candidates_1, reference_1grams)['f']\n",
        "          if(s > 1):\n",
        "            candidates_2 = set(evaluated_1grams[i:i+s-1])\n",
        "            rouge +=  cal_rouge(candidates_2, reference_2grams)['f']\n",
        "          if rouge > max_rouge:\n",
        "              max_rouge = rouge\n",
        "              start = i\n",
        "              end = i+s\n",
        "              text = \"\".join(tokenized_doc[i:i+s])\n",
        "\n",
        "  return start, end, text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icKdxL6osM2t"
      },
      "source": [
        "## Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wECO1WorsPHn"
      },
      "source": [
        "!gdown --id 1-8IU8qyry-yPXwQ7AXz0GHIgn19QKGZP\n",
        "!gdown --id 1-J0eqf4ig7cP8bMPRgSFUejshnBFTZoq\n",
        "!gdown --id 1-IIJFl4AGNr7rRax4YSQTTm7j12YJ0ya"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dps3yrIUsQ-P"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('thaisum.csv')\n",
        "val_df = pd.read_csv('validation_set.csv')\n",
        "test_df = pd.read_csv('test_set.csv')\n",
        "df = pd.concat([df, val_df, test_df], axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDEsL05ttZAP"
      },
      "source": [
        "df = df.reset_index(drop=True)\n",
        "df['body'][358868+11000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQ2iLdT9tcSw"
      },
      "source": [
        "def gold_summary(df, num_train, num_val, num_test):\n",
        "  return df.iloc[num_train+num_val:num_train+num_val+num_test,:]['summary'].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_6q1PNDtdh3"
      },
      "source": [
        "def get_tokenized_df(df):\n",
        "  df = df.reset_index(drop=True)\n",
        "  res = pd.DataFrame(columns=['attention_mask', 'input_ids', 'start_positions', 'end_positions'])\n",
        "  for i in tqdm(range(len(df))):\n",
        "    sent1 = df['body'][i].lower()\n",
        "    sent2 = df['summary'][i].lower()\n",
        "    start, end, _ = selection_start_end_r1_r2(sent1, sent2, tokenizer)\n",
        "    inp_ids = tokenizer(df['body'][i], max_length=416, truncation=True, padding='max_length')['input_ids']\n",
        "    att_mask = tokenizer(df['body'][i], max_length=416, truncation=True, padding='max_length')['attention_mask']\n",
        "    res = res.append({'attention_mask': att_mask, \n",
        "                      'input_ids': inp_ids, \n",
        "                      'start_positions': start, \n",
        "                      'end_positions': end}, ignore_index=True)\n",
        "  return res\n",
        "  '''\n",
        "  return {'input_ids': res['input_ids'].tolist(),\n",
        "          'attention_mask': res['attention_mask'].tolist(),\n",
        "          'start_positions': res['start_positions'].tolist(),\n",
        "          'end_positions': res['end_positions'].tolist()}\n",
        "  '''\n",
        "\n",
        "def get_tokenized_dict(df, num_train, num_val, num_test):\n",
        "  train_df = df.iloc[:num_train, :]\n",
        "  val_df = df.iloc[num_train:num_train+num_val, :]\n",
        "  test_df = df.iloc[num_train+num_val:num_train+num_val+num_test, :]\n",
        "  return {'train': get_tokenized_df(train_df),\n",
        "          'validation': get_tokenized_df(val_df),\n",
        "          'test': get_tokenized_df(test_df)}\n",
        "\n",
        "def get_tokenized_dict_test_val(df, num_train, num_val, num_test):\n",
        "  val_df = df.iloc[num_train:num_train+num_val, :]\n",
        "  test_df = df.iloc[num_train+num_val:num_train+num_val+num_test, :]\n",
        "  return {'validation': get_tokenized_df(val_df),\n",
        "          'test': get_tokenized_df(test_df)}\n",
        "\n",
        "def get_tokenized_dict_test(df, num_train, num_val, num_test):\n",
        "  test_df = df.iloc[num_train+num_val:num_train+num_val+num_test, :]\n",
        "  return {'test': get_tokenized_df(test_df)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjYrfdGntevX"
      },
      "source": [
        "tokenize_with_space([df['body'][369868]], tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRTgv-rCtzJB"
      },
      "source": [
        "Usually tokenizing takes a lot of time, you can choose to tokenize only some part of data by uncommenting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO1R-OI4tmYB"
      },
      "source": [
        "# %%time\n",
        "tokenized_datasets = get_tokenized_dict(df, 358868, 11000, 11000)\n",
        "# tokenized_datasets = get_tokenized_dict_test_val(df, 358868, 11000, 11000)\n",
        "# tokenized_datasets = get_tokenized_dict_test(df, 358868, 11000, 11000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB6G_l3jtlBI"
      },
      "source": [
        "gold_summaries = gold_summary(df, 358868, 11000, 11000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNyDfNgfuL7K"
      },
      "source": [
        "You can choose to save the data after preprocessing and load it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z50S_ACQuK_n"
      },
      "source": [
        "# tokenized_datasets['train'].to_json('train.json', orient='records', lines=True)\n",
        "# tokenized_datasets['validation'].to_json('/content/drive/MyDrive/validation_true_set.json', orient='records', lines=True)\n",
        "# tokenized_datasets['test'].to_json('/content/drive/MyDrive/test_true_lower.json', orient='records', lines=True)\n",
        "tokenized_datasets = load_dataset('json', data_files={'train': '/content/drive/MyDrive/train.json', 'validation': '/content/drive/MyDrive/validation_true.json', 'test': '/content/drive/MyDrive/test_true.json'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhyY5XguuaGZ"
      },
      "source": [
        "tokenized_datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfR-tOmKubLw"
      },
      "source": [
        "#8 in datasets['validation'] points to both 8 and 9 in tokenized_datasets['validation'] due to overflowing tokens\n",
        "i = 8\n",
        "example = tokenized_datasets['validation'][i]\n",
        "combined_text = tokenizer.decode(example['input_ids'])\n",
        "answer_with_token_idx = tokenizer.decode(example['input_ids'][example['start_positions']:example['end_positions']])\n",
        "\n",
        "#there are quite a few more \n",
        "len(tokenized_datasets['validation']), answer_with_token_idx, combined_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sOriBVtud--"
      },
      "source": [
        "## Fine-tuning model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpQqUHj0ui2X"
      },
      "source": [
        "model = AutoModelForQuestionAnswering.from_pretrained(\n",
        "            f'airesearch/{model_name}' if model_name not in public_models else f'{model_name}',\n",
        "            revision='main',)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmbXqOrVuon_"
      },
      "source": [
        "batch_size = 16\n",
        "learning_rate = 4e-5\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"finetune_thaiSum\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=learning_rate,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size*2,\n",
        "    num_train_epochs=6,\n",
        "    warmup_ratio=0.15,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        "    save_total_limit=3,\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "data_collator = default_data_collator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osaFyEdousf_"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5VNrlcSutsH"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pvBytBtuunD"
      },
      "source": [
        "trainer.save_model(\"/content/drive/MyDrive/finetune_thaiSum4\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEyvQ7aFuv6o"
      },
      "source": [
        "## Postprocess and metrics(BERTscore since rouge we already import at the beginning)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trWrMKHJu3jM"
      },
      "source": [
        "def post_process_index(data, raw_predictions, tokenizer, n_best_size = 20, max_answer_length=50):\n",
        "  all_start_logits, all_end_logits = raw_predictions\n",
        "  predictions = []\n",
        "  for start_logits, end_logits, example in zip(all_start_logits, all_end_logits, data):\n",
        "    start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "    end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "    valid_answers = []\n",
        "    for start_index in start_indexes:\n",
        "      for end_index in end_indexes:\n",
        "          # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
        "          if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
        "              continue\n",
        "          valid_answers.append(\n",
        "              {\n",
        "                  \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                  \"text\": tokenizer.decode(example['input_ids'][start_index+1:end_index+1], skip_special_tokens=True)\n",
        "              }\n",
        "          )\n",
        "    if len(valid_answers) > 0:\n",
        "        best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
        "    else:\n",
        "        # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid failure.\n",
        "        best_answer = {\"text\": \"\", \"score\": 0.0} \n",
        "    predictions.append(best_answer[\"text\"])\n",
        "  return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-mvYj2Cu5YQ"
      },
      "source": [
        "### BERTScore"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPeG8OFKu7zI"
      },
      "source": [
        "!pip install bert_score==0.3.7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBus_vjvu9sA"
      },
      "source": [
        "from bert_score import score\n",
        "import numpy as np\n",
        "import gc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG_wz2hcu-zI"
      },
      "source": [
        "def cal_bert_score(cands, refs, get_average_f1=True):\n",
        "  '''\n",
        "  arguments: cands, refs\n",
        "  return array of presicion, recall, f1, presicion_average, recall_average, f1_average\n",
        "  if get_average == True return mean of BERTScore\n",
        "  '''\n",
        "  p, r, f1 = score(cands, refs, lang=\"others\", verbose=False)\n",
        "  p = p.numpy()\n",
        "  r = r.numpy()\n",
        "  f1 = f1.numpy()\n",
        "  if(get_average_f1==True): return f1.mean()\n",
        "  else: return p, r, f1\n",
        "\n",
        "def cal_batch_bert_score(cands, refs, get_average_f1=True, batch_size=8):\n",
        "  f1_average = []\n",
        "  for i in tqdm(range(0,len(cands),batch_size)):\n",
        "    cand_batch = cands[i:i+batch_size]\n",
        "    ref_batch = refs[i:i+batch_size]\n",
        "    res = cal_bert_score(cand_batch, ref_batch)\n",
        "    f1_average.append(res)\n",
        "    gc.collect()\n",
        "  print(f1_average)\n",
        "  return sum(f1_average)/len(f1_average)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JFFLujBvEQs"
      },
      "source": [
        "%%time\n",
        "refs = ['เมื่อวันที่ 6 ม.ค.60 ที่ทำเนียบรัฐบาล นายวิษณุ เครืองาม รองนายกรัฐมนตรี กล่าวถึงกรณี ที่ นายสุรชัย เลี้ยงบุญเลิศชัย รองประธานสภานิติบัญญัติแห่งชาติ (สนช.) ออกมาระบุว่า การเลือกตั้งจะถูกเลื่อนออกไปถึงปี 2561 ว่า ขอให้ไปสอบถามกับ สนช. แต่เชื่อว่าคงไม่กล้าพูดอีก เพราะทำให้คนเข้าใจผิด ซึ่งที่ สนช.พูดเนื่องจากผูกกับกฎหมายของกรรมการร่างรัฐธรรมนูญ(กรธ.) ตนจึงไม่ขอวิพากษ์วิจารณ์ แต่รัฐบาลยืนยันว่ายังเดินตามโรดแม็ป ซึ่งโรดแม็ปมองได้สองแบบ คือ มีลำดับขั้นตอนและการกำหนดช่วงเวลา โดยเริ่มต้นจากการประกาศใช้รัฐธรรมนูญ แต่ขณะนี้รัฐธรรมนูญยังไม่ประกาศใช้ จึงยังเริ่มนับหนึ่งไม่ถูก จากนั้นเข้าสู่ขั้นตอนการร่างกฎหมายประกอบร่างรัฐธรรมนูญหรือกฎหมายลูก ภายใน 240 วัน ก่อนจะส่งกลับให้ สนช.พิจารณา ภายใน 2 เดือน\\xa0,นายวิษณุ กล่าวต่อว่า หากมีการแก้ไขก็จะมีการพิจารณาร่วมกับ กรธ.อีก 1 เดือน ก่อนนำขึ้นทูลเกล้าฯ ทรงลงพระปรมาภิไธย ภายใน 90 วัน และจะเข้าสู่การเลือกตั้งภายในระยะเวลา 5 เดือน ซึ่งทั้งหมดนี้คือโรดแม็ปที่ยังเป็นแบบเดิมอยู่ ส่วนเดิมที่กำหนดวันเลือกตั้งไว้ภายในปี 60 นั้น เพราะมาจากสมมติฐานของขั้นตอนเดิมทั้งหมด แต่เมื่อมีเหตุสวรรคตทุกอย่างจึงต้องเลื่อนออกไป ส่วนการพิจารณากฎหมายลูกทั้งหมด 4 ฉบับ ขณะนี้กรธ.พิจารณาแล้วเสร็จ 2 ฉบับ คือ พ.ร.ป.พรรคการเมือง และพ.ร.ป. คณะกรรมการการเลือกตั้ง แต่ พ.ร.ป.การเลือกตั้งควรจะพิจารณาได้เร็วกลับล่าช้า ดังนั้น กรธ.จะต้องออกชี้แจงถึงเหตุผลว่าทำไมพิจารณากฎหมายดังกล่าวล่าช้ากว่ากำหนด ส่งผลให้เกิดข้อสงสัยจนถึงทุกวันนี้ ส่วนกรณีที่ สนช. ระบุว่า มีกฎหมายเข้าสู่การพิจารณาของ สนช.เป็นจำนวนมาก ทำให้ส่งผลกระทบต่อโรดแม็ปนั้น รัฐบาลเคยบอกไว้แล้วว่าในช่วงนี้ของโรดแม็ปกฎหมายจะเยอะกว่าที่ผ่านมา ดังนั้น สนช.จะต้องบริหารจัดการกันเอง เพราะได้มีการเพิ่มสมาชิก สนช.ให้แล้ว.']\n",
        "cands = ['เมื่อวันที่ 6 ม.ค.60 ที่ทำเนียบรัฐบาล นายวิษณุ เครืองาม รองนายกรัฐมนตรี กล่าวถึงกรณี ที่ นายสุรชัย เลี้ยงบุญเลิศชัย รองประธานสภานิติบัญญัติแห่งชาติ (สนช.)']\n",
        "f1_average = cal_bert_score(cands, refs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAiLo4pMvH1o"
      },
      "source": [
        "print(f1_average)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub0qBZP3vOXx"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FiNg6I-vQZ8"
      },
      "source": [
        "def evaluate_rouge(cands, refs, tokenizer):\n",
        "  cands_tokenized = tokenize_with_space(cands, tokenizer)\n",
        "  refs_tokenized = tokenize_with_space(refs, tokenizer)\n",
        "  r1, r2, rl = cal_rouge_score(refs_tokenized, cands_tokenized)\n",
        "  return r1, r2, rl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnp-HtPwvRcZ"
      },
      "source": [
        "raw_predictions = trainer.predict(tokenized_datasets['test'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2m0uAEGvXiB"
      },
      "source": [
        "predictions = post_process_index(tokenized_datasets['test'], raw_predictions[0], tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7pH2JzdvYMh"
      },
      "source": [
        "predictions[:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eF-Ov9VvaZq"
      },
      "source": [
        "display(predictions[:3], gold_summaries[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_bIxyG_vdr7"
      },
      "source": [
        "r1, r2, rl = evaluate_rouge(predictions, gold_summaries, tokenizer)\n",
        "print(r1, r2, rl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptu9GptvvfGy"
      },
      "source": [
        "%%time\n",
        "BERTScore = cal_batch_bert_score(predictions, gold_summaries, tokenizer, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY5KLvnBvk_p"
      },
      "source": [
        "print(BERTScore)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}